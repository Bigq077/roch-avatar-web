# Theorem Health & Wellness - Interactive Avatar

Production-ready conversational avatars for clinic and rehabilitation support.

## ğŸ¯ Features

- **Clinic Avatar** â€” Mark's digital presence for explaining services, answering questions, encouraging bookings
- **Rehab Avatar** â€” Exercise guide for supporting patients between sessions
- **Low Latency** â€” Cached responses + sentence chunking for 1-2s perceived latency
- **Safety First** â€” Emergency detection, medical boundaries, clear escalation
- **Production Ready** â€” Error handling, fallbacks, monitoring

## ğŸ“ Structure

```
avatar/
â”œâ”€â”€ backend/          # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”‚   â”œâ”€â”€ prompts/             # System prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ clinic_avatar.txt
â”‚   â”‚   â”‚   â””â”€â”€ rehab_avatar.txt
â”‚   â”‚   â”œâ”€â”€ knowledge/           # Clinic data
â”‚   â”‚   â”‚   â””â”€â”€ clinic_config.json
â”‚   â”‚   â””â”€â”€ routes/              # API routes
â”‚   â”‚       â”œâ”€â”€ heygen.py        # HeyGen integration
â”‚   â”‚       â””â”€â”€ stream.py        # Streaming (future)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â””â”€â”€ frontend/         # Vite + Vanilla JS
    â”œâ”€â”€ index.html
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ main.js              # Entry point
    â”‚   â”œâ”€â”€ avatar.js            # HeyGen SDK wrapper
    â”‚   â”œâ”€â”€ ui.js                # UI updates
    â”‚   â””â”€â”€ queue.js             # Message queue
    â”œâ”€â”€ package.json
    â””â”€â”€ vite.config.js
```

## ğŸš€ Quick Start

### 1. Backend Setup

```bash
cd avatar/backend

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp .env.example .env

# Edit .env with your values:
# - BRAIN_API_URL=https://your-brain.onrender.com
# - HEYGEN_API_KEY=xxx
# - ELEVENLABS_VOICE_ID_CLINIC=xxx
# - ELEVENLABS_VOICE_ID_REHAB=xxx

# Run server
uvicorn app.main:app --reload --port 8000
```

### 2. Frontend Setup

```bash
cd avatar/frontend

# Install dependencies
npm install

# Create .env file
cp .env.example .env

# Edit .env:
# - VITE_API_URL=http://localhost:8000
# - VITE_HEYGEN_AVATAR_ID_CLINIC=xxx
# - VITE_HEYGEN_AVATAR_ID_REHAB=xxx

# Run dev server
npm run dev
```

Visit http://localhost:5173

## ğŸ”§ Configuration

### Backend Environment Variables

```env
# Brain API (your existing receptionist brain)
BRAIN_API_URL=https://your-brain.onrender.com

# HeyGen
HEYGEN_API_KEY=your_api_key
HEYGEN_AVATAR_ID_CLINIC=clinic_avatar_id
HEYGEN_AVATAR_ID_REHAB=rehab_avatar_id

# ElevenLabs (voice cloning)
ELEVENLABS_VOICE_ID_CLINIC=mark_voice_id
ELEVENLABS_VOICE_ID_REHAB=neutral_voice_id

# OpenAI (fallback for development)
OPENAI_API_KEY=your_openai_key
OPENAI_MODEL=gpt-4o-mini  # Fast model
USE_OPENAI_FALLBACK=false

# Streaming (future)
ENABLE_STREAMING=false

# Frontend
FRONTEND_URL=http://localhost:5173
```

### Frontend Environment Variables

```env
VITE_API_URL=http://localhost:8000
VITE_HEYGEN_AVATAR_ID_CLINIC=your_clinic_avatar_id
VITE_HEYGEN_AVATAR_ID_REHAB=your_rehab_avatar_id
```

## ğŸ§ª Testing

### Test Backend Health

```bash
curl http://localhost:8000/health
```

Expected response:
```json
{
  "status": "healthy",
  "brain_url": "https://...",
  "heygen_configured": true
}
```

### Test Chat Endpoint

```bash
curl -X POST http://localhost:8000/api/heygen/chat \
  -H "Content-Type: application/json" \
  -d '{
    "mode": "clinic",
    "message": "How much does a physio session cost?"
  }'
```

Expected: Response with `response`, `chunks`, `safety` fields

### Test Cached Responses (instant)

Common questions return cached responses for 0ms latency:
- "How much does it cost?"
- "What are your hours?"
- "Where are you located?"
- "What's your cancellation policy?"
- "Do you take insurance?"

## ğŸ“¦ Deployment

### Backend (Render/Railway)

1. Connect your repo
2. Set environment variables in dashboard
3. Deploy

### Frontend (Vercel/Netlify)

1. Connect your repo
2. Set `VITE_API_URL` to your backend URL
3. Set avatar IDs
4. Deploy

## âš¡ Latency Optimization

Current setup achieves **1-2s perceived latency**:

1. **Cached responses** â€” Common questions return instantly (0ms)
2. **Fast model** â€” Using `gpt-4o-mini` instead of `gpt-4o` (saves 0.5-1s)
3. **Sentence chunking** â€” Avatar starts speaking after first sentence
4. **Reduced tokens** â€” `max_tokens=300` instead of 500

### Future: Streaming (0.5-0.8s latency)

Enable streaming by:
1. Adding streaming endpoint to brain API
2. Set `ENABLE_STREAMING=true`
3. Avatar starts speaking after first tokens

## ğŸ›¡ï¸ Safety Features

### Emergency Detection
Automatically detects and escalates:
- Chest pain
- Severe shortness of breath
- Neurological symptoms
- Anything urgent

### Medical Boundaries
- No diagnosis
- No treatment changes
- Always emphasize assessment first
- Clear escalation to human contact

### Conversation Safeguards
- Emergency banner displays when triggered
- Response validation
- Error handling with graceful fallbacks

## ğŸ¨ Customization

### Adjust Response Length

In `backend/app/routes/heygen.py`:
```python
max_tokens=300  # Change this
```

### Adjust Chunk Size

In `backend/app/routes/heygen.py`:
```python
if len(current_chunk) + len(sentence) < 120:  # Change this
```

### Add Cached Responses

In `backend/app/routes/heygen.py`, update `CACHED_RESPONSES`:
```python
CACHED_RESPONSES = {
    "your_key": "Your instant response here"
}
```

### Modify Avatar Quality

In `backend/app/routes/heygen.py`:
```python
"quality": "high"  # or "medium", "low"
```

## ğŸ“Š Architecture

```
User â†’ Frontend (Vite)
         â†“
      HeyGen SDK
         â†“
   Avatar Backend (FastAPI)
         â†“
   Brain API (Render) â†’ LLM
         â†“
   Response â†’ Chunks â†’ HeyGen â†’ User
```

## ğŸ› Troubleshooting

**"Brain API not responding"**
- Check `BRAIN_API_URL` is correct
- Verify brain endpoint exists at `/api/brain/query`
- Test with curl

**"HeyGen session failed"**
- Verify `HEYGEN_API_KEY`
- Check avatar IDs are correct
- Ensure HeyGen SDK loaded in HTML

**"Slow responses"**
- Check if cached responses working
- Verify using `gpt-4o-mini` not `gpt-4o`
- Consider enabling streaming

## ğŸ“ Development Workflow

1. **Local development** â€” Use `USE_OPENAI_FALLBACK=true` to develop without brain API
2. **Integration testing** â€” Connect to brain API on Render
3. **Production** â€” Deploy both backend and frontend

## ğŸ” Security Notes

- Never commit `.env` files
- Keep API keys secure
- Use environment variables for all secrets
- Enable CORS only for your frontend domain in production

## ğŸ“ Support

For issues or questions:
1. Check the troubleshooting section
2. Review backend logs
3. Test endpoints with curl
4. Check browser console for frontend errors

## âœ… Pre-Launch Checklist

- [ ] Brain endpoint deployed and tested
- [ ] HeyGen avatars created (clinic + rehab)
- [ ] ElevenLabs voices cloned
- [ ] Backend deployed with all env vars
- [ ] Frontend deployed and connected
- [ ] Cached responses tested
- [ ] Emergency handling tested
- [ ] Tested on mobile devices
- [ ] Session timeout handling tested

## ğŸš€ You're Ready!

Start the backend, start the frontend, click "Start Avatar", and you're live!
